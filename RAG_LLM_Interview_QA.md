# 大模型与RAG面试题问答

## 一、RAG流程与编码模型

### Q1: 介绍RAG流程

**A:** RAG（检索增强生成）流程包含以下步骤：

1. **文档切分**：将原始文档按chunk_size切分为小块
2. **向量编码**：使用Embedding模型将文本块编码为向量
3. **向量存储**：将向量存入向量数据库（如Milvus、Chroma）
4. **Query编码**：用户查询经过同一编码模型转为向量
5. **相似度检索**：在向量库中检索Top-K相似文档
6. **上下文拼接**：将检索结果与Query拼接为Prompt
7. **LLM生成**：大模型基于上下文生成最终答案

---

### Q2: 介绍对编码模型的了解、原理、优缺点

**A:**

**原理：**
- 基于Transformer架构（BERT、BGE、M3E等）
- 通过对比学习（Contrastive Learning）训练
- 将文本映射到高维向量空间，语义相似的文本距离更近
- 常用双塔结构：Query和Document分别编码

**优点：**
- 推理速度快，支持离线预计算索引
- 语义理解能力强，超越关键词匹配
- 支持跨语言检索（多语言模型）

**缺点：**
- 长文本截断导致信息损失
- 领域迁移效果下降，需要微调
- 对稀有词、专业术语理解有限
- 向量维度固定，表达能力有上限

---

### Q3: 如何评估编码模型的能力

**A:**

**通用基准：**
- **MTEB**（Massive Text Embedding Benchmark）：涵盖检索、分类、聚类、重排序等8类任务

**检索指标：**
- Recall@K：Top-K结果中包含正确答案的比例
- MRR（Mean Reciprocal Rank）：正确答案排名的倒数均值
- NDCG：考虑排序位置的归一化折损累积增益

**语义相似度：**
- Spearman相关系数
- Pearson相关系数

---

## 二、RAG分类与多模态RAG

### Q4: RAG有哪些分类

**A:**

| 类型 | 特点 |
|------|------|
| **Naive RAG** | 基础检索+生成，简单拼接上下文 |
| **Advanced RAG** | 增加查询改写、重排序、混合检索 |
| **Modular RAG** | 模块化设计，各组件可插拔替换 |
| **Graph RAG** | 结合知识图谱，支持多跳推理 |
| **Agentic RAG** | Agent驱动，自主决策检索策略 |

---

### Q5: 多模态RAG有哪些实现框架

**A:**

- **LlamaIndex**：支持多模态索引和检索
- **LangChain**：通过MultiModal组件扩展
- **Unstructured**：专注多模态文档解析
- **ColPali**：基于视觉语言模型的文档检索
- **VisRAG**：端到端视觉RAG框架

---

### Q6: 伪多模态RAG和多模态RAG分别怎么实现，有什么区别

**A:**

| 维度 | 伪多模态RAG | 真多模态RAG |
|------|-------------|-------------|
| **实现方式** | 图片→OCR/Caption→文本→文本检索 | 图片直接编码为向量，统一向量空间检索 |
| **编码模型** | 文本Embedding模型 | 多模态Embedding模型（CLIP等） |
| **信息保留** | 转换过程有信息损失 | 保留原始视觉语义 |
| **依赖** | 依赖OCR/Caption质量 | 端到端，无中间转换 |
| **适用场景** | 文档类图片（表格、文字） | 自然图像、复杂视觉内容 |

---

### Q7: CLIP可以用于哪一类多模态RAG，为什么

**A:**

CLIP适用于**真多模态RAG**的检索阶段。

**原因：**
- CLIP通过对比学习将图像和文本映射到**同一向量空间**
- 支持**跨模态相似度计算**：文本Query可直接检索图片，反之亦然
- 无需将图片转换为文本，保留完整视觉语义
- 预训练数据量大（4亿图文对），泛化能力强

---

## 三、RAG评估

### Q8: RAG怎么评估

**A:**

**检索质量评估：**
- Recall@K、Precision@K
- MRR（Mean Reciprocal Rank）
- Hit Rate

**生成质量评估：**
- **Faithfulness**（忠实度）：答案是否基于检索内容
- **Relevance**（相关性）：答案是否回答了问题
- **Answer Correctness**：答案正确性

**端到端评估框架：**
- **RAGAS**：自动化RAG评估框架
- **TruLens**：可观测性+评估
- **LangSmith**：追踪+评估

---

### Q9: RAG评估体系中最重要的是什么

**A:**

**最重要的是：Faithfulness（忠实度）**

原因：
- 衡量生成内容是否**基于检索到的上下文**
- 直接反映RAG是否有效利用了检索结果
- 是区分RAG与纯LLM生成的核心指标
- 低忠实度意味着**幻觉风险高**，RAG失去意义

---

## 四、GraphRAG

### Q10: 传统RAG有什么痛点

**A:**

1. **缺乏全局理解**：只能检索局部片段，无法把握文档整体
2. **多跳推理困难**：无法处理需要关联多个信息点的问题
3. **实体关系丢失**：切分后实体间关系被割裂
4. **语义漂移**：检索结果可能偏离真实意图
5. **冗余与噪声**：Top-K结果可能包含重复或无关内容

---

### Q11: 介绍GraphRAG

**A:**

GraphRAG是微软提出的知识图谱增强RAG方案：

**核心流程：**
1. **实体抽取**：从文档中抽取实体和关系
2. **图谱构建**：构建知识图谱
3. **社区检测**：对图谱进行层次化社区划分
4. **社区摘要**：为每个社区生成摘要
5. **检索生成**：结合图结构进行检索和推理

**优势：**
- 支持全局性问题（如"文档主要讲什么"）
- 支持多跳推理
- 保留实体关系

---

### Q12: GraphRAG的难点是什么

**A:**

1. **实体/关系抽取质量**：依赖LLM抽取，准确率和一致性难保证
2. **构建成本高**：需要大量LLM调用，Token消耗大
3. **图谱规模问题**：大规模图的存储和检索效率
4. **实体对齐**：同一实体不同表述的合并
5. **社区划分策略**：层次和粒度的选择影响效果
6. **实时性差**：图谱更新成本高

---

### Q13: GraphRAG如何应对增量场景

**A:**

1. **增量实体抽取**：仅对新文档进行抽取
2. **实体对齐与去重**：新实体与已有实体匹配合并
3. **局部图更新**：只更新受影响的子图区域
4. **社区动态调整**：增量更新社区划分，避免全量重算
5. **摘要增量更新**：仅重新生成受影响社区的摘要
6. **版本管理**：保留图谱历史版本，支持回溯

---

## 五、大模型微调

### Q14: 介绍微调负责的工作

**A:**

1. **数据准备**：收集、清洗、格式化训练数据
2. **数据质量评估**：去重、过滤低质量样本
3. **基座模型选型**：根据任务选择合适的预训练模型
4. **训练策略设计**：选择微调方式（全参/LoRA等）
5. **超参数调优**：学习率、batch_size、epoch等
6. **训练监控**：loss曲线、过拟合检测
7. **模型评估**：在验证集和测试集上评估效果
8. **模型部署**：导出、量化、上线

---

### Q15: 大模型微调最重要的是什么

**A:**

**最重要的是：数据质量**

原因：
- 高质量数据决定微调效果的**上限**
- "Garbage in, garbage out"
- 数据多样性影响模型泛化能力
- 数据与目标任务的**对齐程度**直接影响效果
- 少量高质量数据 > 大量低质量数据

---

## 六、后训练与微调方式

### Q16: 后训练有哪些方式

**A:**

| 方式 | 说明 |
|------|------|
| **SFT** | 监督微调，使用指令-回复对训练 |
| **RLHF** | 人类反馈强化学习，训练奖励模型+PPO优化 |
| **DPO** | 直接偏好优化，无需奖励模型 |
| **ORPO** | 结合SFT和偏好优化的单阶段方法 |
| **KTO** | 基于Kahneman-Tversky优化 |

---

### Q17: 微调有哪些方式，分别是怎么做的

**A:**

| 方式 | 做法 | 特点 |
|------|------|------|
| **全参数微调** | 更新模型所有参数 | 效果最好，资源消耗最大 |
| **LoRA** | 冻结原参数，插入低秩矩阵A、B | 参数高效，效果接近全参 |
| **QLoRA** | LoRA + 4bit量化基座模型 | 显存占用极低 |
| **Adapter** | 在Transformer层间插入小型网络 | 模块化，可插拔 |
| **Prefix Tuning** | 学习可训练的前缀向量 | 不修改模型结构 |
| **P-Tuning v2** | 在每层添加可训练前缀 | 效果优于v1 |

---

### Q18: LoRA原理及参数量

**A:**

**原理：**
- 冻结预训练权重W（d×k维）
- 插入两个低秩矩阵：A（d×r）和B（r×k）
- 前向传播：h = Wx + BAx
- 只训练A和B，r远小于d和k

**参数量计算：**
```
原矩阵参数量：d × k
LoRA参数量：r × (d + k)

示例：d=4096, k=4096, r=8
原参数：16,777,216
LoRA参数：8 × (4096 + 4096) = 65,536
压缩比：约256倍
```

**常用r值：** 8、16、32、64

---

## 七、DPO与PPO

### Q19: 介绍DPO

**A:**

**DPO（Direct Preference Optimization）：**

- 直接从人类偏好数据学习，**无需训练奖励模型**
- 将RLHF的目标函数转化为**分类损失**
- 输入：(prompt, chosen_response, rejected_response)三元组
- 通过最大化chosen与rejected的对数概率差来优化

**损失函数：**
```
L_DPO = -log(σ(β(log π(y_w|x) - log π(y_l|x) - log π_ref(y_w|x) + log π_ref(y_l|x))))
```

---

### Q20: DPO与PPO的区别

**A:**

| 维度 | PPO | DPO |
|------|-----|-----|
| **奖励模型** | 需要单独训练 | 不需要 |
| **训练阶段** | 两阶段（RM + RL） | 单阶段 |
| **模型数量** | 4个（Actor, Critic, RM, Ref） | 2个（Policy, Ref） |
| **训练稳定性** | 较复杂，需仔细调参 | 更稳定简单 |
| **计算成本** | 高 | 低 |
| **理论基础** | 强化学习 | 分类/对比学习 |
| **超参数** | 多（clip, GAE等） | 少（主要是β） |

---

## 八、Agent框架

### Q21: 介绍一些Agent的实现框架

**A:**

| 框架 | 特点 |
|------|------|
| **LangChain** | 链式调用，生态丰富 |
| **LangGraph** | 状态图驱动，支持复杂流程 |
| **AutoGPT** | 自主Agent，目标驱动 |
| **CrewAI** | 多Agent协作，角色扮演 |
| **MetaGPT** | 软件公司模拟，多角色协作 |
| **Dify** | 低代码可视化平台 |
| **AutoGen** | 微软出品，多Agent对话 |

---

### Q22: 这些框架有什么区别

**A:**

| 维度 | LangChain | LangGraph | CrewAI | Dify |
|------|-----------|-----------|--------|------|
| **编程范式** | 链式 | 状态图 | 角色定义 | 可视化 |
| **复杂流程** | 有限 | 强 | 中等 | 中等 |
| **多Agent** | 弱 | 支持 | 强 | 支持 |
| **学习曲线** | 中 | 较高 | 低 | 低 |
| **灵活性** | 高 | 最高 | 中 | 低 |
| **适用场景** | 简单链路 | 复杂工作流 | 团队协作 | 快速原型 |

---

### Q23: LangGraph适用于什么场景

**A:**

1. **复杂多分支流程**：需要条件判断、多路径选择
2. **循环迭代**：需要反复执行直到满足条件
3. **人机交互**：流程中需要人工介入审批
4. **状态管理**：需要维护复杂的中间状态
5. **可恢复执行**：支持断点续跑
6. **多Agent协作**：多个Agent之间的协调调度

---

### Q24: LangGraph构建Agent的方式有哪几种

**A:**

**1. 函数式构建：**
```python
from langgraph.graph import StateGraph

graph = StateGraph(State)
graph.add_node("agent", agent_node)
graph.add_node("tools", tool_node)
graph.add_edge("agent", "tools")
graph.add_conditional_edges("tools", should_continue)
```

**2. 类式构建：**
```python
class MyGraph(StateGraph):
    def __init__(self):
        super().__init__(State)
        self.add_node(...)
```

**3. Prebuilt预置模板：**
```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(model, tools)
```

---

## 九、场景题

### Q25: 客户输入软件/网页界面截图，如何通过RAG帮助用户了解每个组件的作用

**A:**

**输入定义：**
- 用户上传的界面截图（PNG/JPG）

**输出定义：**
```json
{
  "components": [
    {
      "name": "搜索框",
      "bbox": [100, 50, 300, 80],
      "description": "用于输入关键词搜索内容",
      "interactions": ["点击激活", "输入文字", "回车搜索"]
    }
  ]
}
```

**实现方案：**

```
┌─────────────────────────────────────────────────────────┐
│                    离线索引阶段                          │
├─────────────────────────────────────────────────────────┤
│  1. 收集UI组件库（按钮、输入框、下拉菜单等）截图          │
│  2. 为每个组件编写功能描述                               │
│  3. 使用CLIP/SigLIP编码组件图片为向量                    │
│  4. 存入向量数据库（向量 + 组件元数据）                   │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│                    在线推理阶段                          │
├─────────────────────────────────────────────────────────┤
│  1. 用户上传界面截图                                     │
│  2. 目标检测模型（YOLO/GroundingDINO）识别组件区域       │
│  3. 裁剪每个组件区域为独立图片                           │
│  4. CLIP编码组件图片 → 向量检索 → 获取相似组件描述       │
│  5. VLM（GPT-4V）结合检索结果生成最终说明                │
└─────────────────────────────────────────────────────────┘
```

---

### Q26: 相似组件如图片框和视频框如何区分

**A:**

**1. 视觉特征差异：**
- 视频框通常有：播放按钮▶、进度条、时长标识、音量图标
- 图片框通常有：缩放图标、图片轮播指示器

**2. 元数据增强：**
```json
{
  "component_type": "video_player",
  "visual_markers": ["play_button", "progress_bar", "duration"],
  "confidence_boost": 0.2
}
```

**3. 多特征融合检索：**
- 主体区域向量 + 图标检测结果
- 检索时加权：`score = 0.7 * visual_sim + 0.3 * icon_match`

**4. 细粒度编码模型：**
- 使用包含细微差异样本的数据微调CLIP
- 构建对比样本对：(图片框, 视频框)

**5. 后处理规则：**
```python
if detect_play_button(component_image):
    component_type = "video_player"
elif detect_image_indicator(component_image):
    component_type = "image_viewer"
```
